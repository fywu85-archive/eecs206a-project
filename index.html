<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    <style>

        div.padded {
            padding-top: 0px;
            padding-right: 100px;
            padding-bottom: 0.25in;
            padding-left: 100px;
        }

        body {
            padding: 100px;
            width: 900px;
            margin: auto;
            text-align: justify;
            text-justify: inter-word;
            font-weight: 300;
            font-family: 'Helvetica', sans-serif;
            word-wrap: normal;
            white-space: normal;
            color: #121212;
        }

        h1, h2, h3, h4 {
            font-family: 'Helvetica', sans-serif;
        }

        code {
            font-family: 'Liberation Mono';
            font-size: 100%;
            background-color: #FFFFCC;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    </style>
    <title>EECS 206 Final Project</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
    <h1 align="middle">
        Tracking and Rehabilitation of Scapulohumeral Rhythm: <br />
        <i>A Robotic Perspective</i>
    </h1>

    <h2>1 Introduction</h2>
    <p>
        <b>Objective</b>. The objective of this project is to prototype a
        physical therapy system <em>1</em>) for tracking posture of a patient
        with dysfunctional scapulohumeral rhythm and <em>2</em>) for
        automatically designing a  rehabilitation workout plan for fast
        recovery.
    </p>

    <p>
        <b>Motivations</b>.
        The project is interesting in that it uses the model of a robotic arm
        to represent and to plan the motion of scapulohumeral joints.
        To model scapulohumeral joints, one needs
        <em>1</em>) to device a sensor that can measure the patient's posture in
        real time,
        <em>2</em>) to calibrate a robotic arm model using sensor data, and finally
        <em>3</em>) to plan a
        set of meaningful joint actions using the calibrated model.
    </p>

    <p>
        <b>Applications</b>. A direct application of such system is
        to design physical therapy curriculum for patients with
        weak scapulohumeral joints. Extensions of the project include designing
        exo-skeleton to aid patients with weak muscular ability, evaluating
        athletes' form and performance for effective training, and studying
        how muscles and joints recover over time from a dysfunctional state.
    </p>

    <h2>2 Design</h2>

    <p>
        <b>Design criteria</b>. A successful design should be able to accurately
        measure posture of the patient, to fit the measured data into a sufficiently
        expressive model, and to plan a meaningful set of scapulohumeral actions
        based on the calibrated model.
    </p>

    <p>
        <b>Design overview</b>. To measure the subject's body posture, we choose to
        use a Microsoft Azure Kinect camera, which comes with built-in
        library for real-time human pose estimation. To model the data,
        we choose to use a 12-DOF robotic arm (see Figure 1), with a 3-DOF rotation
        at chest, clavicle, shoulder, and elbow, respectively. To plan scapulohumeral
        actions for rehabilitation workout, we assume a naive training principle:
        to exercise joints that are not used often so as to maintain overall joint
        flexibility and strength.
    </p>
    <figure>
        <img src="images/scapulohumeral_joint.png" width="450" class="center">
        <figcaption align="center">
            <b>Figure 1.</b> 12-DOF scalpulohumeral joint model.
        </figcaption>
    </figure>

    <p>
        A schematic diagram of the proposed design is illustrated in Figure 2.
    </p>
    <figure>
        <img src="images/system_design.png" width="850" class="center">
        <figcaption align="center">
            <b>Figure 2.</b> Ideal system design.
        </figcaption>
    </figure>

    <p>
        <b>Choices and tradeoffs</b>. For <em>sensing</em>, we choose to prototype our
        application offline in Python rather than to do it in real time in C++
        for the sake of ease of development, since the camera SDK is only supported
        in C++. For <em>modelling</em>, we assume that each joint can freely
        rotate in
        <img src="http://latex.codecogs.com/gif.latex?\mathbb{S}^{3}" border="0"/>
        and that the motion of each joint is independent from other joints.
        While such modelling choice is easy to implement and is the most expressive,
        it does not prevent the system from generating unrealistic body postures.
        To <em>plan</em> for scapulohumeral movements, we develop an online probabilistic
        planner, which again is easy to implement but will occasionally produce
        unrealistic postures.
    </p>

    <p>
        <b>Impacts of design choices</b>. Developing in Python, while fast for
        prototyping, is less computationally efficient than in C++ for deployment.
        An overly expressive model is less robust to noisy sensor measurement
        since it is difficult to distinguish model errors from irregular data patterns.
        In addition, our planning model has the problem of balancing between retaining
        information from past data and updating model distribution with new data,
        which makes it hard to apply in a general setup.
    </p>

    <h2>3 Implementation</h2>
    <p>
        <b>Hardware and parts</b>. We use a Microsoft Azure Kinect camera to track body
        posture of the subject, as illustrated in Figure 2.
        The only other <em>parts</em> we use are a tripod and a clean
        background (to ensure stable visual tracking).
    </p>

    <p>
        <b>Software</b>. We use the Kinect SDK body tracking to estimate posture
        of the subject. To model the scapulohumeral joint and to implement the
        planning algorithm, we write everything from scratches, using basic
        scientific computing libraries in Python such as Numpy, Scipy, and Matplotlib.
        The architecture of our software implementation is illustrated in Figure 3.
    </p>
    <figure>
        <img src="images/software.png" width="250" class="center">
        <figcaption align="center">
            <b>Figure 3.</b> Software architecture.
        </figcaption>
    </figure>

    <p>
        <b>System integration</b>. The system works as follows:
        <ol>
            <li>Use Kinect SDK to record a sequence of body posture from a subject.</li>
            <li>Load the collected Kinect data into the 12-DOF arm model.</li>
            <li>Compute joint angles at every time step and find the distribution
                of each joint.</li>
            <li>Invert each of the 12 distributions such that a angle with
                probability density in the original distribution will have low
                probability density in the inverted distribution.</li>
            <li>Sample a joint from each of the 12 <em>inverted</em> distributions.</li>
            <li>Compute the final body posture from the 12 sampled joint angle
                using forward kinematics.</li>
        </ol>
    </p>

    <h2>4 Results</h2>
    <p>
        We are able to stream data from a Kinect camera, to build a
        12-DOF robotic arm model using Numpy and Scipy, and to illustrate
        performance of a preliminary pose planner. A high-level summary of our findings
        is provided below.
    </p>
    <p>
        A video of the body tracking camera using the Kinect C++ SDK is shown in
        Video 1. We stream the Kinect camera by a predefined number of frames and save
        the estimated body posture at each frame to a text file for downstream
        calibration and planning.
    </p>
    <figure>
        <img src="images/kinect_compressed.gif" width="500" class="center">
        <figcaption align="center">
            <b>Video 1.</b> Kinect camera body tracking.
        </figcaption>
    </figure>
    <p>
        The calibrated 12-DOF robotic arm is visualized in Video 2, where the
        arm segments are colored in magenta with Euler orientations plotted
        at each joint. Note the data played in Video 2 is using a different
        source video from Video 1.
    </p>
    <figure>
        <img src="images/kinematics.gif" width="600" class="center">
        <figcaption align="center">
            <b>Video 2.</b> Visualization of collected data.
        </figcaption>
    </figure>
    <p>
        From the model, we compute the angular displacement, velocity, and
        acceleration of each joint at every frame and use the data to construct
        time series plots for each of the three attributes of
        the 12 joints in the 12-DOF arm. For example, we show the plots for the
        three Euler-directional joints at the right clavicle in Figure 4.
    </p>
    <figure>
        <img src="images/clavicle_timeseries.png" width="600" class="center">
        <figcaption align="center">
            <b>Figure 4.</b> Time series of angular displacement, velocity,
            and acceleration of the three joints along Euler axes at the right
            clavicle. Note that the Euler angles are with respect to a fixed
            coordinate attached to the camera.
        </figcaption>
    </figure>
    <p>
        As shown in the plots above, the raw velocity and acceleration estimates
        are unrealistically high, but the angular displacement estimates appear
        to be reasonable. In a separate unit test, we find the <em>translational</em>
        displacement error of the Kinect camera is approximately 5 mm.
    </p>
    <p>
        Taking the angular displacement estimates, we are able to construct
        probability density functions (PDF) and inverted PDFs for each of the
        three Euler axes, as shown in Figure 5.
    </p>
    <figure>
        <img src="images/clavicle_hist.png" width="600" class="center">
        <figcaption align="center">
            <b>Figure 5.</b> PDFs (in blue) and inverted PDFs (in orange) of
            angular displacements of the three joints along Euler axes at the
            right clavicle.
        </figcaption>
    </figure>
    <p>
        From the plots above, we find that the recommended orange distribution,
        is inversely related to the blue distribution formed from the data.
        Sampling from the orange distributions will <em>smooth out</em> unevenness
        of the blue distribution over time: if an angular position is less likely
        to be performed by the patient,
        we will recommend more of that position in the workout plan;
        if an angular position is more likely to be performed,
        we will recommend less of that position in the workout plan. Therefore,
        by performing a number of actions recommended by the orange distributions,
        we will eventually achieve an uniform usage of all angular positions at
        every joint in the scapulohumeral region.
    </p>
    <p>
        Finally, we demonstrate a recommended pose in Figure 6, which is sampled
        from the orange distributions and is rendered from forward kinematics.
    </p>
    <figure>
        <img src="images/sample_pose.png" width="640" class="center">
        <figcaption align="center">
            <b>Figure 6.</b> A randomly generated pose used for joint workout
            of the right scapulohumeral region.
        </figcaption>
    </figure>
    <p>
        The result is generated by running <code>python sample_pose.py</code>.
        Note that only the right arm is planned. Other parts of the body is
        assumed a static position in this study.
        Interestingly, we observe a similarity between the generated pose in
        Figure 6 to the common stretch pose of a sitting side reach in Figure 7.
    </p>
    <figure>
        <img src="images/chairlateral.jpg" width="200" class="center">
        <figcaption align="center">
            <b>Figure 7.</b> Sitting Side Reach.
        </figcaption>
    </figure>

    <h2>5 Conclusion</h2>
    <p>
        <b>Discussions</b>. As shown above, we have demonstrated the feasibility
        of the project and have met the basic design criteria specified in Section 2.
        Specifically, we have built a pipeline that streams data
        from a Kinect camera, calibrates a arm model with the data, and finally
        plans joint actions with the data and the calibrated model.
        Meanwhile, we also have a better understanding of the limitations
        in our current implementation. We will continue working on the project
        beyond the scope of the course to resolve them and will provide
        a follow-up section after the conclusion.
        We hope to provide a polish demo to be displayed in
        Rob's lab website, which may serve as a basis for scientific outreach
        and potential future publications.
    </p>

    <p>
        <b>Difficulties</b>. The most difficult part of the project is model
        calibration, where we need to use the observed data to infer the internal
        structure of the 12-DOF robot arm.
        Due to under-documentation, it took me a very long time to understand the
        exact meaning of the Kinect data and therefore to obtain the correct angular
        positions. In addition, it is also very hard to estimate
        angular velocity and acceleration, as shown by the unrealistic velocity
        and acceleration time series in Figure 4. To mitigate this problem, we can
        build a dynamics model of the arm and apply noise reduction using extended
        Kalman filter.
    </p>
    <p>
        It is also a challenge to ensure that the recommended pose is always
        natural and physically feasible. Because the model we use does not consider
        correlations among joints, it is possible to generate actions that look
        awkward or even impossible for a normal person to perform. To impose joint
        correlation, we can either encode it in a dynamics model, or use a
        hidden Markov model to capture joint dependency during the sampling step.
    </p>

    <h2>6 Follow-Up Study</h2>
    <p>
        <b>Issues</b>. From post-project testing, I learned that I have initially
        interpreted the Kinect joint orientation data incorrectly. I was able to
        identify a parsing issue with the orientation quaternion: the quaternion
        rotation library in Scipy does not produce the right conversion.
        Instead, another library, pytransform3d, gives the right conversion
        after a 180 degree rotation about the y axis.
        Consequently, the visualizations shown in Figure 6 and Figure 7 in Section
        4 were slightly off and have since been updated to reflect this correction.
    </p>
    <p>
        <b>Improvements</b>. Lastly, I have explored another idea proposed by Rob,
        that is, to plan in task space rather than in configuration space. This is
        motivated by the fact that planning in task space has the advantage of
        always being able to produce natural actions.
    </p>
    <p>
        To achieve this, we adopt an interpolation approach. We first collect a
        set of data that contains positions in task space as labels and the
        corresponding joint configurations as inputs. By training a neural
        network model to fit the data, we can then freely sample any position in the
        task space and use the trained neural network to compute the corresponding
        joint configuration. To this end, we have collected a set of data and
        trained a prototype neural network on the data. The resulting training
        curve is shown in Figure 8 with the model's predictions on the <em>training data</em>
        visualized in Figure 9.
    </p>
    <figure>
        <img src="images/interpolate_pose_loss.png" width="640" class="center">
        <figcaption align="center">
            <b>Figure 8.</b> Training Curve.
        </figcaption>
    </figure>
    <figure>
        <img src="images/interpolate_pose_scatter.png" width="640" class="center">
        <figcaption align="center">
            <b>Figure 9.</b> Projected Predictions of the Trained Model.
        </figcaption>
    </figure>

    <p>
        The result is generated by running
        <code>python prepare_endpose.py && python interpolate_pose.py</code>.
        As indicated in Figure 8 and Figure 9, the neural network, despite reasonable
        convergence during training, still cannot fully fit the training data.
        We hypothesize that there are two possible sources of errors:
        <em>1</em>) The network is too small to fully encode all information in the dataset.
        <em>2</em>) The data itself may not be generated by a well-conditioned function:
        it is possible that for highly similar inputs, the corresponding outputs
        are very different, therefore causing difficulties in model fitting.
    </p>
    <h2>7 Team</h2>
    <p>
        Fangyu Wu is the contributor of this course project and is working with
        Robert Matthew at UCSF.
        Fangyu is a second year PhD student in EECS with research interests in
        biologically inspired, distributed, robotic systems.
    </p>

    <h2>8 Additional Materials</h2>
    <p>
        The code and data used in this project is available
        <a href="https://github.com/fywu85/eecs206a_project">here</a>.
        The presentation slides can be viewed
        <a href="https://docs.google.com/presentation/d/1IS0hZxmA9OpBZ8LyjvTmE-Yrehz61JBvbgEsZOodsxA/edit?usp=sharing">here</a>.
        Note that this report contains follow-up updates that are not reflected
        in the presentation slides.
    </p>
</body>
</html>
